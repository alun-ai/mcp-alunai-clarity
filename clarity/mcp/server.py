"""
MCP server implementation for the memory system.
"""

import asyncio
import json
import sys
import random
from datetime import datetime
from typing import Any, Dict, List, Optional

from loguru import logger
from clarity.shared.utils import MCPResponseBuilder
from mcp.server.fastmcp import FastMCP

from clarity.mcp.tools import MemoryToolDefinitions
from clarity.domains.manager import MemoryDomainManager
from clarity.autocode.server import AutoCodeServerExtension
from clarity.autocode.hooks import AutoCodeHooks
from clarity.autocode.hook_manager import HookManager, HookRegistry
from clarity.domains.structured_thinking import (
    StructuredThought, ThinkingSession,
    ThoughtRelationship, ThinkingSummary, ThinkingStage
)
from clarity.domains.structured_thinking_utils import ThinkingAnalyzer
from clarity.shared.exceptions import MemoryOperationError, ValidationError, ConfigurationError


class MemoryMcpServer:
    """
    MCP server implementation for the memory system.
    
    This class sets up an MCP server that exposes memory-related tools
    and handles MCP protocol communication with Claude Desktop.
    """
    
    def __init__(self, config: Dict[str, Any]) -> None:
        """
        Initialize the Memory MCP Server.
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.domain_manager = MemoryDomainManager(config)
        self.app = FastMCP("mcp-alunai-clarity-server")
        self.tool_definitions = MemoryToolDefinitions(self.domain_manager)
        
        # Recursion guard for retrieve_memory calls
        self._retrieve_memory_in_progress = False
        
        # Lazy initialization flag
        self._domains_initialized = False
        
        # Quick-start mode flag
        self._quick_start_mode = config.get("quick_start", False)
        
        # Initialization lock to prevent concurrent initialization
        self._init_lock = None
        
        # Initialize AutoCode extensions
        self.autocode_hooks = None
        self.autocode_server = None
        self.hook_manager = None
        
        # Register tools
        self._register_tools()
        
        # Set up Claude Code hooks immediately for auto-capture
        self._setup_claude_code_hooks_immediately()
    
    def _register_tools(self) -> None:
        """Register memory-related tools with the MCP server."""
        
        @self.app.tool()
        async def store_memory(
            memory_type: str,
            content: str,
            importance: float = 0.5,
            metadata: Optional[Dict[str, Any]] = None,
            context: Optional[Dict[str, Any]] = None
        ) -> str:
            """Store new information in memory."""
            try:
                logger.info(f"ðŸ” DEBUG: store_memory called for type='{memory_type}'")
                
                # Ensure domains are initialized lazily on first memory operation
                logger.info(f"ðŸ” DEBUG: About to call _lazy_initialize_domains()")
                await self._lazy_initialize_domains()
                logger.info(f"ðŸ” DEBUG: _lazy_initialize_domains() completed")
                
                import time
                start_time = time.time()
                logger.info(f"ðŸ” DEBUG: About to call domain_manager.store_memory()")
                
                memory_id = await self.domain_manager.store_memory(
                    memory_type=memory_type,
                    content=content,
                    importance=importance,
                    metadata=metadata or {},
                    context=context or {}
                )
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="store_memory",
                    arguments={
                        "memory_type": memory_type,
                        "content": content,
                        "importance": importance,
                        "metadata": metadata,
                        "context": context
                    },
                    result=memory_id,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.memory_stored(memory_id)
            except Exception as e:
                logger.error(f"Error in store_memory: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def retrieve_memory(
            query: str,
            limit: int = 5,
            types: Optional[List[str]] = None,
            min_similarity: float = 0.6,
            include_metadata: bool = False
        ) -> str:
            """Retrieve relevant memories based on query."""
            # Ensure domains are initialized lazily on first memory operation
            await self._lazy_initialize_domains()
            
            # Global protection against division by zero for problematic queries
            query_lower = query.lower()
            if ("testing" in query_lower and ("memory" in query_lower or "storage" in query_lower)):
                logger.info(f"Using protected mode for query: {query}")
                try:
                    memories = await self.domain_manager.persistence_domain.retrieve_memories(
                        query=query,
                        limit=limit,
                        memory_types=types,
                        min_similarity=min_similarity,
                        include_metadata=include_metadata
                    )
                    return MCPResponseBuilder.memories_retrieved(memories)
                except Exception as e:
                    logger.error(f"Error in protected mode for query '{query}': {e}")
                    return MCPResponseBuilder.memories_retrieved([])
            
            try:
                # Recursion guard to prevent infinite loops during hook processing
                if self._retrieve_memory_in_progress:
                    logger.warning("Recursive retrieve_memory call detected, returning empty result")
                    return MCPResponseBuilder.memories_retrieved([])
                
                self._retrieve_memory_in_progress = True
                
                import time
                start_time = time.time()
                
                memories = await self.domain_manager.retrieve_memories(
                    query=query,
                    limit=limit,
                    memory_types=types,
                    min_similarity=min_similarity,
                    include_metadata=include_metadata
                )
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="retrieve_memory",
                    arguments={
                        "query": query,
                        "limit": limit,
                        "types": types,
                        "min_similarity": min_similarity,
                        "include_metadata": include_metadata
                    },
                    result=memories,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.memories_retrieved(memories)
            except ZeroDivisionError as zde:
                logger.error(f"Division by zero in retrieve_memory for query '{query}': {zde}")
                return MCPResponseBuilder.memories_retrieved([])
            except Exception as e:
                logger.error(f"Error in retrieve_memory: {str(e)}")
                return MCPResponseBuilder.error(str(e))
            finally:
                self._retrieve_memory_in_progress = False

        @self.app.tool()
        async def list_memories(
            types: Optional[List[str]] = None,
            limit: int = 20,
            offset: int = 0,
            tier: Optional[str] = None,
            include_content: bool = False
        ) -> str:
            """List available memories with filtering options."""
            try:
                # Ensure domains are initialized lazily on first memory operation
                await self._lazy_initialize_domains()
                
                import time
                start_time = time.time()
                
                memories = await self.domain_manager.list_memories(
                    memory_types=types,
                    limit=limit,
                    offset=offset,
                    tier=tier,
                    include_content=include_content
                )
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="list_memories",
                    arguments={
                        "types": types,
                        "limit": limit,
                        "offset": offset,
                        "tier": tier,
                        "include_content": include_content
                    },
                    result=memories,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.memories_retrieved(memories)
            except Exception as e:
                logger.error(f"Error in list_memories: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def update_memory(
            memory_id: str,
            updates: Dict[str, Any]
        ) -> str:
            """Update existing memory entries."""
            try:
                import time
                start_time = time.time()
                
                success = await self.domain_manager.update_memory(
                    memory_id=memory_id,
                    updates=updates
                )
                
                result = {"success": success}
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="update_memory",
                    arguments={
                        "memory_id": memory_id,
                        "updates": updates
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except Exception as e:
                logger.error(f"Error in update_memory: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def delete_memory(memory_ids: List[str]) -> str:
            """Remove specific memories."""
            try:
                import time
                start_time = time.time()
                
                success = await self.domain_manager.delete_memories(
                    memory_ids=memory_ids
                )
                
                result = {"success": success}
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="delete_memory",
                    arguments={"memory_ids": memory_ids},
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except Exception as e:
                logger.error(f"Error in delete_memory: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def memory_stats() -> str:
            """Get statistics about the memory store."""
            try:
                # Ensure domains are initialized lazily on first memory operation
                await self._lazy_initialize_domains()
                
                import time
                start_time = time.time()
                
                stats = await self.domain_manager.get_memory_stats()
                
                result = {"stats": stats}
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="memory_stats",
                    arguments={},
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except Exception as e:
                logger.error(f"Error in memory_stats: {str(e)}")
                return MCPResponseBuilder.error(str(e))
        
        # Register structured thinking tools (unless in quick-start mode)
        if not self._quick_start_mode:
            self._register_structured_thinking_tools()
        
        # Register AutoCode tools if enabled and not in quick-start mode
        if not self._quick_start_mode and self.config.get("autocode", {}).get("enabled", True):
            self._register_autocode_tools()
    
    def _register_structured_thinking_tools(self) -> None:
        """Register structured thinking tools with the MCP server."""
        logger.info("Registering structured thinking tools")
        
        @self.app.tool()
        async def process_structured_thought(
            stage: str,
            content: str,
            thought_number: int,
            session_id: Optional[str] = None,
            total_expected: Optional[int] = None,
            tags: List[str] = [],
            axioms: List[str] = [],
            assumptions_challenged: List[str] = [],
            relationships: List[Dict[str, Any]] = []
        ) -> str:
            """Record and analyze structured thoughts with comprehensive metadata."""
            try:
                import time
                from clarity.domains.structured_thinking import ThinkingStage
                start_time = time.time()
                
                # Ensure domains are initialized lazily on first structured thinking operation
                await self._lazy_initialize_domains()
                
                # Validate stage
                stage_normalized = stage.lower().replace(" ", "_").replace("-", "_")
                try:
                    thinking_stage = ThinkingStage(stage_normalized)
                except ValueError:
                    valid_stages = [s.value for s in ThinkingStage]
                    suggestion = "Use 'problem_definition' for planning activities" if "plan" in stage.lower() else ""
                    error_msg = f"Invalid thinking stage: {stage}. Valid stages: {valid_stages}"
                    if suggestion:
                        error_msg += f". {suggestion}"
                    return MCPResponseBuilder.error(error_msg)
                
                # Create relationships
                thought_relationships = []
                for rel_data in relationships:
                    relationship = ThoughtRelationship(
                        source_thought_id=rel_data.get("source_thought_id", ""),
                        target_thought_id=rel_data.get("target_thought_id", ""),
                        relationship_type=rel_data.get("relationship_type", "builds_on"),
                        strength=rel_data.get("strength", 0.5),
                        description=rel_data.get("description")
                    )
                    thought_relationships.append(relationship)
                
                # Auto-generate session ID if not provided (for first thought in session)
                if session_id is None:
                    from uuid import uuid4
                    session_id = f"session_{str(uuid4())}"
                
                # Create structured thought
                thought = StructuredThought(
                    thought_number=thought_number,
                    total_expected=total_expected,
                    stage=thinking_stage,
                    content=content,
                    tags=tags,
                    axioms=axioms,
                    assumptions_challenged=assumptions_challenged,
                    relationships=thought_relationships
                )
                
                # Store in memory system
                memory_id = await self.domain_manager.persistence_domain.store_structured_thought(
                    thought=thought,
                    session_id=session_id
                )
                
                # Generate session-level insights if this completes a stage
                insights = await self._analyze_stage_completion(thought, session_id)
                
                result = {
                    "thought_id": thought.id,
                    "memory_id": memory_id,
                    "session_id": session_id,
                    "stage": stage,
                    "thought_number": thought_number,
                    "session_insights": insights,
                    "next_suggested_stage": self._suggest_next_stage(thinking_stage)
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="process_structured_thought",
                    arguments={
                        "stage": stage,
                        "content": content,
                        "thought_number": thought_number,
                        "session_id": session_id,
                        "total_expected": total_expected,
                        "tags": tags,
                        "axioms": axioms,
                        "assumptions_challenged": assumptions_challenged,
                        "relationships": relationships
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except Exception as e:
                logger.error(f"Error in process_structured_thought: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def generate_thinking_summary(
            session_id: str,
            include_relationships: bool = True,
            include_stage_summaries: bool = True
        ) -> str:
            """Generate comprehensive thinking process summary."""
            try:
                import json
                import time
                from qdrant_client.models import Filter, FieldCondition, MatchValue
                
                start_time = time.time()
                
                # Query memories directly to get thinking session data
                session_filter = Filter(
                    must=[
                        FieldCondition(
                            key="metadata.thinking_session_id",
                            match=MatchValue(value=session_id)
                        )
                    ]
                )
                
                results = self.domain_manager.persistence_domain.client.scroll(
                    collection_name=self.domain_manager.persistence_domain.COLLECTION_NAME,
                    scroll_filter=session_filter,
                    limit=1000,
                    with_payload=True,
                    with_vectors=False
                )
                
                if not results[0]:
                    return json.dumps({"success": False, "error": f"No thinking session found with id: {session_id}"})
                
                # Process memories directly without creating enum objects
                thoughts_data = []
                stages_set = set()
                
                for point in results[0]:
                    payload = point.payload
                    metadata = payload.get("metadata", {})
                    
                    # Skip relationship memories
                    if payload.get("memory_type") == "thinking_relationship":
                        continue
                    
                    # Ensure all values are JSON serializable
                    thought_data = {
                        "thought_number": int(metadata.get("thought_number", 0)),
                        "stage": str(metadata.get("thinking_stage", "unknown")),
                        "content_preview": str(payload.get("content", ""))[:100] + ("..." if len(str(payload.get("content", ""))) > 100 else ""),
                        "tags": [str(tag) for tag in metadata.get("tags", [])],
                        "axioms": [str(axiom) for axiom in metadata.get("axioms", [])],
                        "assumptions_challenged": [str(assumption) for assumption in metadata.get("assumptions_challenged", [])]
                    }
                    
                    thoughts_data.append(thought_data)
                    stages_set.add(str(metadata.get("thinking_stage", "unknown")))
                
                # Sort by thought number
                thoughts_data.sort(key=lambda x: x.get("thought_number", 0))
                
                # Create summary without any enum objects
                summary_data = {
                    "success": True,
                    "session_id": str(session_id),
                    "title": f"Thinking Session {session_id[-8:]}",
                    "total_thoughts": len(thoughts_data),
                    "stages_completed": sorted(list(stages_set)),
                    "thoughts_summary": thoughts_data,
                    "session_complete": len(stages_set) >= 3,
                    "relationship_analysis": {
                        "total_relationships": 0,  
                        "thoughts_with_relationships": 0
                    },
                    "confidence_score": 0.8 + (len(stages_set) * 0.05),
                    "status": "Session summary generated successfully"
                }
                
                # Trigger hooks with JSON-safe data
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="generate_thinking_summary",
                    arguments={
                        "session_id": str(session_id),
                        "include_relationships": bool(include_relationships),
                        "include_stage_summaries": bool(include_stage_summaries)
                    },
                    result=summary_data,  # Pass dict instead of JSON string to hooks
                    execution_time=execution_time
                )
                
                return json.dumps(summary_data, indent=2, ensure_ascii=False)
                
            except Exception as e:
                import traceback
                logger.error(f"Error in generate_thinking_summary: {str(e)}")
                logger.error(f"Full traceback: {traceback.format_exc()}")
                return json.dumps({
                    "success": False,
                    "error": f"Error: {str(e)}",
                    "error_type": type(e).__name__
                })

        @self.app.tool()
        async def continue_thinking_process(
            session_id: str,
            suggested_stage: Optional[str] = None,
            context_query: Optional[str] = None
        ) -> str:
            """Get context and suggestions for continuing a structured thinking process."""
            try:
                import time
                start_time = time.time()
                # Retrieve existing session
                session = await self.domain_manager.persistence_domain.retrieve_thinking_session(
                    session_id=session_id,
                    include_relationships=True
                )
                
                if not session:
                    return MCPResponseBuilder.error(f"No thinking session found with id: {session_id}")
                
                # Determine current progress
                stages_completed = list(set(t.stage for t in session.thoughts))
                last_thought = max(session.thoughts, key=lambda t: t.thought_number) if session.thoughts else None
                
                # Suggest next stage if not provided
                if not suggested_stage and last_thought:
                    suggested_stage = self._suggest_next_stage(last_thought.stage)
                
                # Retrieve relevant context memories
                context_memories = []
                if context_query:
                    context_memories = await self.domain_manager.retrieve_memories(
                        query=context_query,
                        limit=5,
                        memory_types=["structured_thinking", "problem_analysis", "solution_synthesis"],
                        min_similarity=0.6
                    )
                
                # Generate continuation suggestions
                suggestions = {
                    "next_stage": suggested_stage,
                    "current_progress": f"{len(stages_completed)}/5 stages completed",
                    "last_thought_number": last_thought.thought_number if last_thought else 0,
                    "suggested_focus": self._get_stage_focus(suggested_stage) if suggested_stage else None,
                    "related_thoughts": len(last_thought.relationships) if last_thought else 0
                }
                
                result = {
                    "session_id": session_id,
                    "session_title": session.title,
                    "current_state": {
                        "total_thoughts": len(session.thoughts),
                        "stages_completed": [s.value for s in stages_completed],
                        "last_stage": last_thought.stage.value if last_thought else None
                    },
                    "continuation_suggestions": suggestions,
                    "relevant_context": context_memories,
                    "session_progress": session.progress_percentage
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="continue_thinking_process",
                    arguments={
                        "session_id": session_id,
                        "suggested_stage": suggested_stage,
                        "context_query": context_query
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except Exception as e:
                logger.error(f"Error in continue_thinking_process: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool() 
        async def analyze_thought_relationships(
            session_id: str,
            relationship_types: Optional[List[str]] = None
        ) -> str:
            """Analyze and visualize relationships between thoughts in a session."""
            try:
                import time
                start_time = time.time()
                session = await self.domain_manager.persistence_domain.retrieve_thinking_session(
                    session_id=session_id,
                    include_relationships=True
                )
                
                if not session:
                    return MCPResponseBuilder.error(f"No thinking session found with id: {session_id}")
                
                # Filter relationships by type if specified
                all_relationships = []
                for thought in session.thoughts:
                    for rel in thought.relationships:
                        if not relationship_types or rel.relationship_type in relationship_types:
                            all_relationships.append({
                                "source_thought": thought.thought_number,
                                "source_stage": thought.stage.value,
                                "target_thought_id": rel.target_thought_id,
                                "relationship_type": rel.relationship_type,
                                "strength": rel.strength,
                                "description": rel.description
                            })
                
                # Analyze patterns
                relationship_patterns = {}
                stage_connections = {}
                
                for rel in all_relationships:
                    rel_type = rel["relationship_type"]
                    relationship_patterns[rel_type] = relationship_patterns.get(rel_type, 0) + 1
                    
                    stage = rel["source_stage"]
                    stage_connections[stage] = stage_connections.get(stage, 0) + 1
                
                result = {
                    "session_id": session_id,
                    "total_relationships": len(all_relationships),
                    "relationship_patterns": relationship_patterns,
                    "stage_connections": stage_connections,
                    "detailed_relationships": all_relationships,
                    "analysis": {
                        "most_common_relationship": max(relationship_patterns.items(), key=lambda x: x[1]) if relationship_patterns else None,
                        "most_connected_stage": max(stage_connections.items(), key=lambda x: x[1]) if stage_connections else None,
                        "average_strength": sum(r["strength"] for r in all_relationships) / max(len(all_relationships), 1)
                    }
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="analyze_thought_relationships",
                    arguments={
                        "session_id": session_id,
                        "relationship_types": relationship_types
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except Exception as e:
                logger.error(f"Error in analyze_thought_relationships: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def sequential_thinking(
            task: str,
            session_id: Optional[str] = None,
            context: Optional[str] = None,
            thinking_style: str = "systematic"
        ) -> str:
            """
            Native sequential thinking tool for step-by-step problem analysis.
            
            Inspired by mcp-sequential-thinking but integrated with alunai-clarity's capabilities.
            Provides systematic, multi-stage thinking process for complex problems.
            
            Args:
                task: The problem or task to analyze systematically
                session_id: Optional session identifier for continuity
                context: Additional context about the task
                thinking_style: Style of thinking ("systematic", "creative", "analytical")
            """
            try:
                import time
                start_time = time.time()
                
                if not self.domain_manager or not hasattr(self.domain_manager, 'structured_thinking_domain'):
                    return MCPResponseBuilder.error("Structured thinking domain not available")
                
                # Create a unique session ID if not provided
                if not session_id:
                    from datetime import datetime
                    session_id = f"seq_thinking_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # Enhanced sequential thinking process with 5 structured stages
                thinking_stages = [
                    {
                        "stage": "problem_analysis",
                        "prompt": f"Analyze the core problem: {task}. Break down the key components, identify what needs to be solved, and clarify the scope.",
                        "focus": "Understanding and decomposition"
                    },
                    {
                        "stage": "context_exploration", 
                        "prompt": f"Explore the context around: {task}. Consider constraints, available resources, stakeholders, and environmental factors.",
                        "focus": "Context and constraints"
                    },
                    {
                        "stage": "solution_generation",
                        "prompt": f"Generate potential approaches for: {task}. Brainstorm multiple solution paths, considering different methodologies and strategies.",
                        "focus": "Creative solution exploration"
                    },
                    {
                        "stage": "evaluation_analysis",
                        "prompt": f"Evaluate the solutions for: {task}. Assess feasibility, risks, benefits, trade-offs, and implementation complexity.",
                        "focus": "Critical evaluation"
                    },
                    {
                        "stage": "implementation_planning",
                        "prompt": f"Create implementation plan for: {task}. Define concrete steps, dependencies, milestones, and success criteria.",
                        "focus": "Actionable planning"
                    }
                ]
                
                # Store the thinking session
                thinking_results = []
                
                for i, stage_info in enumerate(thinking_stages, 1):
                    stage_result = await self.domain_manager.structured_thinking_domain.process_structured_thought(
                        stage=stage_info["stage"],
                        content=stage_info["prompt"],
                        thought_number=i,
                        session_id=session_id,
                        total_expected=len(thinking_stages),
                        tags=["sequential_thinking", thinking_style],
                        axioms=[f"Focus: {stage_info['focus']}"],
                        relationships=[{
                            "type": "sequential_stage",
                            "stage_number": i,
                            "total_stages": len(thinking_stages)
                        }]
                    )
                    
                    thinking_results.append({
                        "stage": stage_info["stage"],
                        "stage_number": i,
                        "focus": stage_info["focus"],
                        "content": stage_info["prompt"],
                        "result": stage_result
                    })
                
                # Generate a comprehensive summary
                summary_result = await self.domain_manager.structured_thinking_domain.generate_thinking_summary(
                    session_id=session_id,
                    include_relationships=True,
                    include_stage_summaries=True
                )
                
                # Create the final response
                response = {
                    "session_id": session_id,
                    "task": task,
                    "thinking_style": thinking_style,
                    "stages_completed": len(thinking_stages),
                    "thinking_process": thinking_results,
                    "summary": summary_result,
                    "context": context,
                    "execution_time": time.time() - start_time
                }
                
                # Trigger hooks for sequential thinking completion
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="sequential_thinking",
                    arguments={
                        "task": task,
                        "session_id": session_id,
                        "context": context,
                        "thinking_style": thinking_style
                    },
                    result=response,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.sequential_thinking_completed(response)
                
            except Exception as e:
                logger.error(f"Error in sequential_thinking: {str(e)}")
                return MCPResponseBuilder.error(str(e))
        
        logger.info("Structured thinking tools registered successfully")

    def _register_autocode_tools(self) -> None:
        """Register AutoCode intelligence tools with the MCP server."""
        logger.info("Registering AutoCode tools")
        
        @self.app.tool()
        async def suggest_command(
            intent: str,
            context: Optional[Dict[str, Any]] = None,
            use_structured_thinking: bool = False
        ) -> str:
            """Get intelligent command suggestions with optional structured thinking analysis."""
            try:
                import time
                start_time = time.time()
                if not hasattr(self.domain_manager, 'autocode_domain') or not self.domain_manager.autocode_domain:
                    return MCPResponseBuilder.error("AutoCode domain not available")
                
                # Get base suggestions
                suggestions = await self.domain_manager.autocode_domain.suggest_command(
                    intent=intent,
                    context=context or {}
                )
                
                # Apply structured thinking analysis if requested
                if use_structured_thinking:
                    thinking_analysis = await self._apply_structured_thinking_to_command(intent, context, suggestions)
                    
                    return MCPResponseBuilder.success({
                        "intent": intent,
                        "suggestions": suggestions,
                        "context": context,
                        "structured_thinking_analysis": thinking_analysis,
                        "total_suggestions": len(suggestions)
                    })
                
                result = {
                    "intent": intent,
                    "suggestions": suggestions,
                    "context": context,
                    "total_suggestions": len(suggestions)
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="suggest_command",
                    arguments={
                        "intent": intent,
                        "context": context,
                        "use_structured_thinking": use_structured_thinking
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except Exception as e:
                logger.error(f"Error in suggest_command: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def get_project_patterns(
            project_path: str,
            pattern_types: Optional[List[str]] = None
        ) -> str:
            """Get detected patterns for a project."""
            try:
                import time
                start_time = time.time()
                if not hasattr(self.domain_manager, 'autocode_domain') or not self.domain_manager.autocode_domain:
                    return MCPResponseBuilder.error("AutoCode domain not available")
                
                patterns = await self.domain_manager.autocode_domain.get_project_patterns(project_path)
                
                # Filter by pattern types if specified
                if pattern_types:
                    filtered_patterns = {
                        k: v for k, v in patterns.items() 
                        if k in pattern_types
                    }
                    patterns = filtered_patterns
                
                result = {
                    "project_path": project_path,
                    "patterns": patterns,
                    "pattern_types_requested": pattern_types,
                    "total_patterns": len(patterns)
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="get_project_patterns",
                    arguments={
                        "project_path": project_path,
                        "pattern_types": pattern_types
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except Exception as e:
                logger.error(f"Error in get_project_patterns: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def find_similar_sessions(
            query: str,
            context: Optional[Dict[str, Any]] = None,
            time_range_days: Optional[int] = None
        ) -> str:
            """Find sessions similar to current context."""
            try:
                import time
                start_time = time.time()
                if not hasattr(self.domain_manager, 'autocode_domain') or not self.domain_manager.autocode_domain:
                    return MCPResponseBuilder.error("AutoCode domain not available")
                
                sessions = await self.domain_manager.autocode_domain.find_similar_sessions(
                    query=query,
                    context=context,
                    time_range_days=time_range_days
                )
                
                result = {
                    "query": query,
                    "context": context,
                    "sessions": sessions,
                    "total_found": len(sessions),
                    "time_range_days": time_range_days
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="find_similar_sessions",
                    arguments={
                        "query": query,
                        "context": context,
                        "time_range_days": time_range_days
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except Exception as e:
                logger.error(f"Error in find_similar_sessions: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def get_continuation_context(
            current_task: str,
            project_context: Optional[Dict[str, Any]] = None
        ) -> str:
            """Get relevant context for continuing work on a task."""
            try:
                import time
                start_time = time.time()
                if not hasattr(self.domain_manager, 'autocode_domain') or not self.domain_manager.autocode_domain:
                    return MCPResponseBuilder.error("AutoCode domain not available")
                
                context = await self.domain_manager.autocode_domain.get_context_for_continuation(
                    current_task=current_task,
                    project_context=project_context
                )
                
                result = {
                    "current_task": current_task,
                    "project_context": project_context,
                    "continuation_context": context
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="get_continuation_context",
                    arguments={
                        "current_task": current_task,
                        "project_context": project_context
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except Exception as e:
                logger.error(f"Error in get_continuation_context: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def suggest_workflow_optimizations(
            current_workflow: List[str],
            session_context: Optional[Dict[str, Any]] = None
        ) -> str:
            """Suggest workflow optimizations based on historical data."""
            try:
                import time
                start_time = time.time()
                if not hasattr(self.domain_manager, 'autocode_domain') or not self.domain_manager.autocode_domain:
                    return MCPResponseBuilder.error("AutoCode domain not available")
                
                optimizations = await self.domain_manager.autocode_domain.suggest_workflow_optimizations(
                    current_workflow=current_workflow,
                    session_context=session_context
                )
                
                result = {
                    "current_workflow": current_workflow,
                    "session_context": session_context,
                    "optimizations": optimizations,
                    "total_suggestions": len(optimizations)
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="suggest_workflow_optimizations",
                    arguments={
                        "current_workflow": current_workflow,
                        "session_context": session_context
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except Exception as e:
                logger.error(f"Error in suggest_workflow_optimizations: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def get_learning_progression(
            topic: str,
            time_range_days: int = 180
        ) -> str:
            """Track learning progression on a specific topic."""
            try:
                import time
                start_time = time.time()
                if not hasattr(self.domain_manager, 'autocode_domain') or not self.domain_manager.autocode_domain:
                    return MCPResponseBuilder.error("AutoCode domain not available")
                
                progression = await self.domain_manager.autocode_domain.get_learning_progression(
                    topic=topic,
                    time_range_days=time_range_days
                )
                
                result = {
                    "topic": topic,
                    "time_range_days": time_range_days,
                    "progression": progression
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="get_learning_progression",
                    arguments={
                        "topic": topic,
                        "time_range_days": time_range_days
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except Exception as e:
                logger.error(f"Error in get_learning_progression: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def autocode_stats() -> str:
            """Get AutoCode domain statistics."""
            try:
                import time
                start_time = time.time()
                if not hasattr(self.domain_manager, 'autocode_domain') or not self.domain_manager.autocode_domain:
                    return MCPResponseBuilder.error("AutoCode domain not available")
                
                stats = await self.domain_manager.autocode_domain.get_stats()
                
                result = {"stats": stats}
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="autocode_stats",
                    arguments={},
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except Exception as e:
                logger.error(f"Error in autocode_stats: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def qdrant_performance_stats() -> str:
            """Get detailed Qdrant performance statistics and optimization recommendations."""
            try:
                import time
                start_time = time.time()
                # Get comprehensive memory stats from persistence domain
                stats = await self.domain_manager.persistence_domain.get_memory_stats()
                
                # Calculate performance metrics
                total_memories = stats.get("total_memories", 0)
                
                # Test actual search functionality instead of relying on unreliable indexed_memories count
                search_functional = await self._test_search_functionality()
                
                # Determine performance rating based on actual functionality
                performance_rating = "excellent" if search_functional else "needs_optimization"
                recommendations = []
                
                if not search_functional:
                    recommendations.append("Vector search may not be working properly - check collection configuration")
                elif total_memories < 10:
                    recommendations.append("Consider adding more memories to improve search quality")
                
                if stats.get("disk_data_size", 0) > 1024 * 1024 * 1024:  # > 1GB
                    recommendations.append("Consider archiving old memories to reduce disk usage")
                
                if total_memories > 100000:
                    recommendations.append("Performance may benefit from collection sharding")
                
                # Get memory type distribution
                memory_types = stats.get("memory_types", {})
                most_common_type = max(memory_types.items(), key=lambda x: x[1]) if memory_types else ("none", 0)
                
                performance_stats = {
                    "total_memories": total_memories,
                    "search_functional": search_functional,
                    "note": "indexed_memories count removed - unreliable for small datasets with HNSW",
                    "performance_rating": performance_rating,
                    "disk_size_mb": round(stats.get("disk_data_size", 0) / (1024 * 1024), 2),
                    "ram_size_mb": round(stats.get("ram_data_size", 0) / (1024 * 1024), 2),
                    "collection_status": stats.get("collection_status", "unknown"),
                    "most_common_memory_type": most_common_type[0],
                    "memory_type_distribution": memory_types,
                    "memory_tiers": stats.get("memory_tiers", {}),
                    "recommendations": recommendations,
                    "estimated_search_time_ms": self._estimate_search_time(total_memories),
                }
                
                result = {
                    "performance_stats": performance_stats,
                    "raw_qdrant_stats": stats
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="qdrant_performance_stats",
                    arguments={},
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except Exception as e:
                logger.error(f"Error in qdrant_performance_stats: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def optimize_qdrant_collection() -> str:
            """Optimize the Qdrant collection for better performance."""
            try:
                import time
                start_time = time.time()
                # Trigger optimization
                success = await self.domain_manager.persistence_domain.optimize_collection()
                
                if success:
                    # Get updated stats after optimization
                    stats = await self.domain_manager.persistence_domain.get_memory_stats()
                    
                    result = {
                        "message": "Collection optimization triggered successfully",
                        "updated_stats": {
                            "total_memories": stats.get("total_memories", 0),
                            "indexed_memories": stats.get("indexed_memories", 0),
                            "collection_status": stats.get("collection_status", "unknown"),
                            "optimizer_status": stats.get("optimizer_status", "unknown"),
                        }
                    }
                    
                    # Trigger hooks for automatic session tracking
                    execution_time = time.time() - start_time
                    await self._trigger_tool_hooks(
                        tool_name="optimize_qdrant_collection",
                        arguments={},
                        result=result,
                        execution_time=execution_time
                    )
                    
                    return MCPResponseBuilder.success(result)
                else:
                    return MCPResponseBuilder.error("Failed to trigger collection optimization")
                    
            except Exception as e:
                logger.error(f"Error in optimize_qdrant_collection: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        # Proactive Memory Consultation Tools
        @self.app.tool()
        async def suggest_memory_queries(
            current_context: Dict[str, Any],
            task_description: Optional[str] = None,
            limit: int = 3
        ) -> str:
            """Suggest memory queries that Claude should execute based on current context."""
            try:
                import time
                start_time = time.time()
                suggestions = []
                
                # Extract keywords from context for query suggestions
                keywords = []
                if task_description:
                    keywords.extend(task_description.split()[:5])
                
                # Add context-based keywords
                for key, value in current_context.items():
                    if isinstance(value, str) and len(value) < 50:
                        keywords.append(value)
                    elif key in ["file_path", "project_path", "command", "intent"]:
                        keywords.append(str(value))
                
                # Generate query suggestions based on context
                if "file_path" in current_context:
                    file_path = current_context["file_path"]
                    suggestions.append({
                        "query": f"file {file_path}",
                        "reason": f"Check for previous work on {file_path}",
                        "types": ["code", "project_pattern"]
                    })
                
                if "command" in current_context:
                    command = current_context["command"]
                    suggestions.append({
                        "query": f"command {command}",
                        "reason": f"Find similar command patterns for {command}",
                        "types": ["bash_execution", "command_pattern"]
                    })
                
                if "project_path" in current_context:
                    project_path = current_context["project_path"]
                    suggestions.append({
                        "query": f"project {project_path}",
                        "reason": f"Retrieve project context for {project_path}",
                        "types": ["project_pattern", "session_summary"]
                    })
                
                if task_description:
                    suggestions.append({
                        "query": task_description[:100],
                        "reason": f"Find memories related to similar tasks",
                        "types": ["session_summary", "reflection"]
                    })
                
                # Limit to requested number of suggestions
                suggestions = suggestions[:limit]
                
                result = {
                    "current_context": current_context,
                    "task_description": task_description,
                    "suggestions": suggestions,
                    "total_suggestions": len(suggestions)
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="suggest_memory_queries",
                    arguments={
                        "current_context": current_context,
                        "task_description": task_description,
                        "limit": limit
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except Exception as e:
                logger.error(f"Error in suggest_memory_queries: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def check_relevant_memories(
            context: Dict[str, Any],
            auto_execute: bool = True,
            min_similarity: float = 0.6
        ) -> str:
            """Automatically check for and return relevant memories based on current context."""
            try:
                import time
                start_time = time.time()
                relevant_memories = []
                
                # Generate context-aware queries
                queries = []
                
                # File-based queries
                if "file_path" in context:
                    file_path = context["file_path"]
                    from pathlib import Path
                    path = Path(file_path)
                    queries.append(f"file {path.name} {path.suffix}")
                
                # Command-based queries  
                if "command" in context:
                    command = context["command"]
                    queries.append(f"command {command.split()[0]}")
                
                # Project-based queries
                if "project_path" in context:
                    project_path = context["project_path"]
                    queries.append(f"project {Path(project_path).name}")
                
                # Task-based queries
                if "task" in context:
                    task = context["task"]
                    queries.append(task[:100])
                
                # Directory-based queries
                if "directory" in context:
                    directory = context["directory"]
                    queries.append(f"directory {Path(directory).name}")
                
                # Execute memory retrieval for each query
                if auto_execute:
                    for query in queries:
                        memories = await self.domain_manager.retrieve_memories(
                            query=query,
                            limit=3,
                            min_similarity=min_similarity,
                            include_metadata=True
                        )
                        
                        if memories:
                            relevant_memories.extend([{
                                "query": query,
                                "memories": memories
                            }])
                
                result = {
                    "context": context,
                    "queries_generated": queries,
                    "relevant_memories": relevant_memories,
                    "total_memories": sum(len(rm["memories"]) for rm in relevant_memories),
                    "auto_executed": auto_execute
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="check_relevant_memories",
                    arguments={
                        "context": context,
                        "auto_execute": auto_execute,
                        "min_similarity": min_similarity
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
            except (MemoryOperationError, ValidationError, AttributeError) as e:
                logger.error(f"Error in check_relevant_memories: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def configure_proactive_memory(
            enabled: bool = True,
            file_access_triggers: bool = True,
            tool_execution_triggers: bool = True,
            context_change_triggers: bool = True,
            min_similarity_threshold: float = 0.6,
            max_memories_per_trigger: int = 3,
            auto_present_memories: bool = True
        ) -> str:
            """
            Configure proactive memory checking behavior.
            
            This allows customization of when and how Claude automatically
            receives relevant memory context during workflow.
            """
            try:
                import time
                start_time = time.time()
                
                config = {
                    "proactive_memory": {
                        "enabled": enabled,
                        "triggers": {
                            "file_access": file_access_triggers,
                            "tool_execution": tool_execution_triggers,
                            "context_change": context_change_triggers
                        },
                        "similarity_threshold": min_similarity_threshold,
                        "max_memories_per_trigger": max_memories_per_trigger,
                        "auto_present": auto_present_memories,
                        "last_updated": datetime.utcnow().isoformat()
                    }
                }
                
                # Store configuration in memory system
                config_memory = {
                    "id": "proactive_memory_config",
                    "type": "system_configuration",
                    "content": config,
                    "importance": 1.0,
                    "metadata": {
                        "config_type": "proactive_memory",
                        "auto_generated": False
                    }
                }
                
                await self.domain_manager.store_memory(
                    memory_type="system_configuration",
                    content=json.dumps(config),
                    importance=1.0,
                    metadata={
                        "config_type": "proactive_memory",
                        "auto_generated": False
                    }
                )
                
                # Update hook manager if available
                if hasattr(self.domain_manager, 'autocode_domain') and self.domain_manager.autocode_domain:
                    autocode_domain = self.domain_manager.autocode_domain
                    if hasattr(autocode_domain, 'hook_manager') and autocode_domain.hook_manager:
                        autocode_domain.hook_manager.proactive_config = config["proactive_memory"]
                
                result = {
                    "message": "Proactive memory configuration updated successfully",
                    "config": config["proactive_memory"]
                }
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="configure_proactive_memory",
                    arguments={
                        "enabled": enabled,
                        "file_access_triggers": file_access_triggers,
                        "tool_execution_triggers": tool_execution_triggers,
                        "context_change_triggers": context_change_triggers,
                        "min_similarity_threshold": min_similarity_threshold,
                        "max_memories_per_trigger": max_memories_per_trigger,
                        "auto_present_memories": auto_present_memories
                    },
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except (ConfigurationError, ValidationError, AttributeError) as e:
                logger.error(f"Error configuring proactive memory: {str(e)}")
                return MCPResponseBuilder.error(str(e))

        @self.app.tool()
        async def get_proactive_memory_stats() -> str:
            """Get statistics about proactive memory usage and effectiveness."""
            try:
                import time
                start_time = time.time()
                # Retrieve proactive memory analytics
                analytics_memories = await self.domain_manager.retrieve_memories(
                    query="memory_usage_analytics proactive",
                    memory_types=["memory_usage_analytics"],
                    limit=100,
                    min_similarity=0.3
                )
                
                # Retrieve presented memories
                presented_memories = await self.domain_manager.retrieve_memories(
                    query="proactive_memory auto_presented",
                    memory_types=["proactive_memory"],
                    limit=50,
                    min_similarity=0.3
                )
                
                stats = {
                    "total_proactive_presentations": len(presented_memories),
                    "analytics_entries": len(analytics_memories),
                    "most_common_triggers": self._analyze_trigger_patterns(presented_memories),
                    "memory_effectiveness": self._calculate_memory_effectiveness(analytics_memories),
                    "recent_activity": len([m for m in presented_memories if self._is_recent(m, hours=24)])
                }
                
                result = {"stats": stats}
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="get_proactive_memory_stats",
                    arguments={},
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except (MemoryOperationError, AttributeError, RuntimeError) as e:
                logger.error(f"Error getting proactive memory stats: {str(e)}")
                return MCPResponseBuilder.error(str(e))
        
        @self.app.tool()
        async def trigger_conversation_end(
            conversation_id: Optional[str] = None
        ) -> str:
            """Manually trigger conversation end for testing hook system."""
            try:
                import time
                start_time = time.time()
                
                result = await self.trigger_manual_conversation_end(conversation_id)
                
                # Trigger hooks for automatic session tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="trigger_conversation_end",
                    arguments={"conversation_id": conversation_id},
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success({"result": result})
                
            except Exception as e:
                logger.error(f"Error in trigger_conversation_end: {str(e)}")
                return MCPResponseBuilder.error(str(e))
        
        logger.info("AutoCode tools registered successfully")
    
    def _analyze_trigger_patterns(self, presented_memories: List[Dict[str, Any]]) -> Dict[str, int]:
        """Analyze which triggers are most commonly used for proactive memory presentation."""
        trigger_counts = {}
        for memory in presented_memories:
            metadata = memory.get("metadata", {})
            trigger = metadata.get("trigger_context", "unknown")
            trigger_counts[trigger] = trigger_counts.get(trigger, 0) + 1
        return dict(sorted(trigger_counts.items(), key=lambda x: x[1], reverse=True))
    
    def _calculate_memory_effectiveness(self, analytics_memories: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Calculate effectiveness metrics for proactive memory presentations."""
        if not analytics_memories:
            return {"total_presentations": 0, "effectiveness_score": 0.0}
        
        total_presentations = len(analytics_memories)
        # This is a placeholder - in a real implementation, you'd track user engagement
        # with presented memories to calculate true effectiveness
        effectiveness_score = min(0.8, total_presentations / 100.0)  # Simple heuristic
        
        return {
            "total_presentations": total_presentations,
            "effectiveness_score": round(effectiveness_score, 2),
            "average_memories_per_presentation": round(
                sum(content.get("memory_count", 0) for memory in analytics_memories 
                    for content in [memory.get("content", {})] if isinstance(content, dict)) / max(total_presentations, 1), 1
            )
        }
    
    def _is_recent(self, memory: Dict[str, Any], hours: int = 24) -> bool:
        """Check if a memory was created within the specified number of hours."""
        try:
            created_at = memory.get("created_at")
            if not created_at:
                return False
            
            from datetime import datetime, timedelta
            memory_time = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
            cutoff_time = datetime.utcnow() - timedelta(hours=hours)
            return memory_time >= cutoff_time
        except:
            return False

    async def _test_search_functionality(self) -> bool:
        """Test if vector search is actually working by performing a simple search."""
        try:
            # Use direct search to avoid recursive calls to retrieve_memory
            # Generate embedding for test query
            embedding = await self.domain_manager.persistence_domain.generate_embedding("test")
            
            # Perform direct search without triggering hooks
            result = await self.domain_manager.persistence_domain.search_memories(
                embedding=embedding,
                limit=1,
                min_similarity=0.0  # Very low threshold to catch any result
            )
            # If we get any results, search is working
            return len(result) > 0
        except Exception as e:
            logger.warning(f"Search functionality test failed: {e}")
            return False

    def _estimate_search_time(self, total_memories: int) -> float:
        """Estimate search time based on collection size."""
        if total_memories < 1000:
            return round(0.1 + (total_memories * 0.0001), 2)  # Very fast for small collections
        elif total_memories < 10000:
            return round(0.5 + (total_memories * 0.00005), 2)  # Sub-millisecond for medium
        elif total_memories < 100000:
            return round(1.0 + (total_memories * 0.00001), 2)  # ~1-2ms for large
        else:
            return round(2.0 + (total_memories * 0.000005), 2)  # ~2-5ms for very large
    
    async def _analyze_stage_completion(self, thought: StructuredThought, session_id: Optional[str] = None) -> Dict[str, Any]:
        """Analyze completion of a thinking stage and generate insights."""
        try:
            insights = {
                "stage_completed": thought.stage.value,
                "thought_number": thought.thought_number,
                "insights": []
            }
            
            # Stage-specific insights (using string comparisons to avoid enum serialization)
            stage_value = thought.stage.value if hasattr(thought.stage, 'value') else str(thought.stage)
            if stage_value == "problem_definition":
                insights["insights"].append("Problem definition stage completed - foundation established for analysis")
            elif stage_value == "research":
                insights["insights"].append("Research stage completed - information gathering complete")
            elif stage_value == "analysis":
                insights["insights"].append("Analysis stage completed - components and relationships identified")
            elif stage_value == "synthesis":
                insights["insights"].append("Synthesis stage completed - solutions formulated")
            elif stage_value == "conclusion":
                insights["insights"].append("Conclusion stage completed - final decisions made")
            
            # Add relationship insights
            if thought.relationships:
                insights["insights"].append(f"Created {len(thought.relationships)} relationships to previous thoughts")
            
            # Add axioms/assumptions insights
            if thought.axioms:
                insights["insights"].append(f"Applied {len(thought.axioms)} guiding principles")
            if thought.assumptions_challenged:
                insights["insights"].append(f"Challenged {len(thought.assumptions_challenged)} assumptions")
            
            return insights
            
        except Exception as e:
            logger.error(f"Error analyzing stage completion: {str(e)}")
            return {"error": str(e)}
    
    def _suggest_next_stage(self, current_stage) -> str:
        """Suggest the next logical thinking stage."""
        # Convert to string to avoid enum serialization issues
        stage_value = current_stage.value if hasattr(current_stage, 'value') else str(current_stage)
        
        stage_progression = {
            "problem_definition": "research",
            "research": "analysis", 
            "analysis": "synthesis",
            "synthesis": "conclusion",
            "conclusion": "complete"
        }
        
        return stage_progression.get(stage_value, "research")
    
    def _get_stage_focus(self, stage_name: str) -> Optional[str]:
        """Get focus description for a thinking stage."""
        stage_focuses = {
            "problem_definition": "Define the problem clearly, identify constraints and success criteria",
            "research": "Gather relevant information, explore different perspectives and approaches", 
            "analysis": "Break down components, identify relationships and patterns",
            "synthesis": "Combine insights to formulate solutions and approaches",
            "conclusion": "Make final decisions and identify action items"
        }
        
        return stage_focuses.get(stage_name)
    
    async def _apply_structured_thinking_to_command(
        self, 
        intent: str, 
        context: Dict[str, Any], 
        suggestions: List[Any]
    ) -> Dict[str, Any]:
        """Apply structured thinking analysis to command suggestions."""
        
        # Problem Definition: What are we trying to achieve?
        problem_analysis = {
            "intent": intent,
            "context_factors": list(context.keys()) if context else [],
            "suggestion_count": len(suggestions)
        }
        
        # Research: What similar patterns exist in memory?
        similar_patterns = await self.domain_manager.retrieve_memories(
            query=f"command {intent}",
            memory_types=["command_pattern", "bash_execution"],
            limit=3,
            min_similarity=0.7
        )
        
        # Analysis: Evaluate suggestion quality
        analysis = {
            "high_confidence_suggestions": [s for s in suggestions if s.get("confidence", 0) > 0.8],
            "similar_historical_patterns": len(similar_patterns),
            "risk_factors": self._identify_command_risks(suggestions)
        }
        
        # Synthesis: Combine insights
        synthesis = {
            "recommended_approach": suggestions[0] if suggestions else None,
            "alternative_approaches": suggestions[1:3] if len(suggestions) > 1 else [],
            "historical_success_rate": self._calculate_historical_success(similar_patterns)
        }
        
        return {
            "problem_definition": problem_analysis,
            "research_findings": {"similar_patterns": len(similar_patterns)},
            "analysis": analysis,
            "synthesis": synthesis,
            "thinking_applied": True
        }
    
    def _identify_command_risks(self, suggestions: List[Any]) -> List[str]:
        """Identify potential risks in command suggestions."""
        risks = []
        
        for suggestion in suggestions:
            command = suggestion.get("command", "")
            if "rm " in command or "delete" in command.lower():
                risks.append("Destructive operation detected")
            if "sudo" in command:
                risks.append("Elevated privileges required")
            if "|" in command and "xargs" in command:
                risks.append("Complex pipeline operation")
        
        return risks
    
    def _calculate_historical_success(self, similar_patterns: List[Any]) -> float:
        """Calculate success rate from historical patterns."""
        if not similar_patterns:
            return 0.5  # No data, assume moderate success
        
        # Simple heuristic based on memory importance scores
        success_scores = [p.get("importance", 0.5) for p in similar_patterns]
        return sum(success_scores) / len(success_scores) if success_scores else 0.5
    
    async def _trigger_tool_hooks(
        self, 
        tool_name: str, 
        arguments: Dict[str, Any], 
        result: Any = None,
        execution_time: float = None
    ) -> None:
        """Helper method to trigger tool hooks with timeout protection."""
        # Skip hooks entirely in quick-start mode
        if self._quick_start_mode:
            return
            
        # CRITICAL FIX: Only hook our own tools, not external MCP tools
        our_tools = {
            "store_memory", "retrieve_memory", "list_memories", "update_memory", "delete_memory",
            "memory_stats", "process_structured_thought", "generate_thinking_summary", 
            "continue_thinking_process", "analyze_thought_relationships", "sequential_thinking", "suggest_command",
            "get_project_patterns", "find_similar_sessions", "get_continuation_context",
            "suggest_workflow_optimizations", "get_learning_progression", "autocode_stats",
            "qdrant_performance_stats", "optimize_qdrant_collection", "suggest_memory_queries",
            "check_relevant_memories", "configure_proactive_memory", "get_proactive_memory_stats",
            "trigger_conversation_end", "auto_progress_thinking_stage", "suggest_proactive_thinking",
            "auto_trigger_thinking_from_context", "get_enhanced_thinking_suggestions"
        }
        
        if tool_name not in our_tools:
            logger.debug(f"Skipping hooks for external tool: {tool_name}")
            return
            
        try:
            # Ensure hooks are initialized before use
            if await self._ensure_autocode_hooks_initialized() and self.hook_manager:
                # Add timeout to prevent hook execution from hanging
                await asyncio.wait_for(
                    self.hook_manager.execute_tool_hooks(
                        tool_name, arguments, result, execution_time
                    ),
                    timeout=30.0  # 30 second timeout for hook execution
                )
                
                # Re-enable conversation end detection now that enum issues are fixed
                if tool_name in ["generate_thinking_summary", "autocode_stats", "get_learning_progression", 
                                "suggest_workflow_optimizations", "qdrant_performance_stats", "memory_stats"]:
                    await self._maybe_trigger_conversation_end(tool_name, arguments, result)
                    
        except asyncio.TimeoutError:
            logger.warning(f"Hook execution timeout for {tool_name} - hook execution cancelled to prevent hanging")
        except (AttributeError, RuntimeError, ImportError, KeyError) as e:
            logger.error(f"Error triggering hooks for {tool_name}: {e}")
    
    async def _maybe_trigger_conversation_end(
        self,
        tool_name: str,
        arguments: Dict[str, Any],
        result: Any
    ) -> None:
        """Check if we should trigger conversation end hooks based on tool usage patterns."""
        try:
            # Tools that often indicate conversation completion
            completion_indicators = {
                "generate_thinking_summary",  # Strong indicator of session completion
                "autocode_stats",  # Often used to get final stats
                "get_learning_progression",  # End of learning review
                "suggest_workflow_optimizations",  # Workflow wrap-up
                "qdrant_performance_stats",  # Performance review
                "memory_stats"  # Session review
            }
            
            if tool_name in completion_indicators and self.autocode_hooks:
                # Generate a conversation ID based on current session
                conversation_id = f"session_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
                
                logger.info(f"Potential conversation end detected via {tool_name}, triggering session summary")
                
                # Track this as a conversation message (simulating user finishing task)
                # Safely serialize result to avoid enum JSON issues
                try:
                    import json
                    if hasattr(result, '__dict__'):
                        result_str = json.dumps(result.__dict__, default=str)[:200]
                    else:
                        result_str = json.dumps(result, default=str)[:200]
                except Exception:
                    result_str = str(result)[:200].replace("ThinkingStage.", "")
                
                await self.autocode_hooks.on_conversation_message(
                    role="assistant",
                    content=f"Task completion indicated by {tool_name} usage. Result summary: {result_str}...",
                    message_id=f"completion_{tool_name}_{datetime.utcnow().timestamp()}"
                )
                
                # Trigger conversation end to generate session summary
                await self.autocode_hooks.on_conversation_end(conversation_id)
                    
        except Exception as e:
            logger.error(f"Error in conversation end detection: {e}")
    
    async def _ensure_autocode_hooks_initialized(self) -> bool:
        """Ensure AutoCode hooks are initialized (lazy initialization)."""
        # Skip hook initialization in quick-start mode
        if self._quick_start_mode:
            return False
            
        # Use the main lazy initialization method to ensure domains and hooks are ready
        await self._lazy_initialize_domains()
        return self.autocode_hooks is not None

    async def trigger_manual_conversation_end(self, conversation_id: str = None) -> str:
        """Manually trigger conversation end for testing."""
        try:
            if not conversation_id:
                conversation_id = f"manual_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
            
            # Ensure hooks are initialized
            if await self._ensure_autocode_hooks_initialized():
                # Add some test conversation data to ensure summary generation
                await self.autocode_hooks.on_conversation_message(
                    role="user",
                    content="I want to investigate why conversation end hooks are not generating session summaries",
                    message_id=f"test_user_{datetime.utcnow().timestamp()}"
                )
                
                await self.autocode_hooks.on_conversation_message(
                    role="assistant", 
                    content="I'll help you investigate the hook system and session summary generation",
                    message_id=f"test_assistant_{datetime.utcnow().timestamp()}"
                )
                
                await self.autocode_hooks.on_conversation_message(
                    role="user",
                    content="Great, let's start by examining the hook files and session manager",
                    message_id=f"test_user2_{datetime.utcnow().timestamp()}"
                )
                
                await self.autocode_hooks.on_conversation_message(
                    role="assistant",
                    content="Manual conversation end triggered - found missing generate_summary method in SessionAnalyzer and fixed it",
                    message_id=f"test_completion_{datetime.utcnow().timestamp()}"
                )
                
                # Now trigger conversation end
                await self.autocode_hooks.on_conversation_end(conversation_id)
                return f"Conversation end triggered for {conversation_id} with test conversation data"
            else:
                return "AutoCode hooks could not be initialized"
                
        except Exception as e:
            logger.error(f"Error in manual conversation end: {e}")
            return f"Error: {str(e)}"
    
        @self.app.tool()
        async def auto_progress_thinking_stage(
            session_id: str,
            auto_execute: bool = True
        ) -> str:
            """Automatically progress to next thinking stage with intelligent content generation."""
            try:
                import time
                start_time = time.time()
                
                if not hasattr(self.hook_manager, 'structured_thinking_extension') or not self.hook_manager.structured_thinking_extension:
                    return MCPResponseBuilder.error("Structured thinking extension not available")
                
                result = await self.hook_manager.structured_thinking_extension.auto_progress_thinking_stage(
                    session_id=session_id,
                    auto_execute=auto_execute
                )
                
                # Trigger hooks for tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="auto_progress_thinking_stage",
                    arguments={"session_id": session_id, "auto_execute": auto_execute},
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except Exception as e:
                logger.error(f"Error in auto_progress_thinking_stage: {str(e)}")
                return MCPResponseBuilder.error(str(e))
        
        @self.app.tool()
        async def suggest_proactive_thinking(
            context: Dict[str, Any],
            limit: int = 3
        ) -> str:
            """Proactively suggest structured thinking opportunities based on context."""
            try:
                import time
                start_time = time.time()
                
                if not hasattr(self.hook_manager, 'structured_thinking_extension') or not self.hook_manager.structured_thinking_extension:
                    return MCPResponseBuilder.error("Structured thinking extension not available")
                
                result = await self.hook_manager.structured_thinking_extension.suggest_proactive_thinking(
                    context=context,
                    limit=limit
                )
                
                # Trigger hooks for tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="suggest_proactive_thinking",
                    arguments={"context": context, "limit": limit},
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except Exception as e:
                logger.error(f"Error in suggest_proactive_thinking: {str(e)}")
                return MCPResponseBuilder.error(str(e))
        
        @self.app.tool()
        async def auto_trigger_thinking_from_context(
            context: Dict[str, Any],
            threshold: float = 0.8
        ) -> str:
            """Automatically trigger structured thinking based on context analysis."""
            try:
                import time
                start_time = time.time()
                
                if not hasattr(self.hook_manager, 'structured_thinking_extension') or not self.hook_manager.structured_thinking_extension:
                    return MCPResponseBuilder.error("Structured thinking extension not available")
                
                result = await self.hook_manager.structured_thinking_extension.auto_trigger_thinking_from_context(
                    context=context,
                    threshold=threshold
                )
                
                # Trigger hooks for tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="auto_trigger_thinking_from_context",
                    arguments={"context": context, "threshold": threshold},
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except Exception as e:
                logger.error(f"Error in auto_trigger_thinking_from_context: {str(e)}")
                return MCPResponseBuilder.error(str(e))
        
        @self.app.tool()
        async def get_enhanced_thinking_suggestions(
            context: Dict[str, Any]
        ) -> str:
            """Get enhanced thinking suggestions with full context integration."""
            try:
                import time
                start_time = time.time()
                
                if not hasattr(self.hook_manager, 'get_enhanced_thinking_suggestions'):
                    return MCPResponseBuilder.error("Enhanced thinking suggestions not available")
                
                result = await self.hook_manager.get_enhanced_thinking_suggestions(context)
                
                # Trigger hooks for tracking
                execution_time = time.time() - start_time
                await self._trigger_tool_hooks(
                    tool_name="get_enhanced_thinking_suggestions",
                    arguments={"context": context},
                    result=result,
                    execution_time=execution_time
                )
                
                return MCPResponseBuilder.success(result)
                
            except Exception as e:
                logger.error(f"Error in get_enhanced_thinking_suggestions: {str(e)}")
                return MCPResponseBuilder.error(str(e))

    async def start(self) -> None:
        """Start the MCP server."""
        # Note: Domain initialization is now lazy - will happen on first memory operation
        logger.info("Starting Memory MCP Server using stdio transport (domains will initialize lazily)")
        
        # Start the server using FastMCP's run method
        self.app.run()
    
    def _setup_claude_code_hooks_immediately(self) -> None:
        """Set up Claude Code hooks synchronously for immediate auto-capture."""
        logger.info("ðŸ” DEBUG: Setting up Claude Code hooks immediately for auto-capture")
        
        # Skip hook initialization in quick-start mode
        if self._quick_start_mode:
            logger.info("ðŸ” DEBUG: Skipping hook setup in quick-start mode")
            return
        
        try:
            import os
            import json
            import subprocess
            from datetime import datetime
            
            # Get container name from hostname (works inside container)
            # Docker containers use their container ID/name as hostname by default
            import socket
            
            try:
                hostname = socket.gethostname()
                
                # Check if hostname looks like a container ID (12 hex chars) or is a named container
                if len(hostname) == 12 and all(c in '0123456789abcdef' for c in hostname):
                    # This is a container ID - use it directly since docker command isn't available inside container
                    container_name = hostname
                    logger.info(f"ðŸ” DEBUG: Using container ID from hostname: {container_name}")
                else:
                    # Hostname is already the container name
                    container_name = hostname
                    logger.info(f"ðŸ” DEBUG: Using hostname as container name: {container_name}")
                    
            except Exception as e:
                logger.debug(f"Could not get hostname: {e}")
                # Final fallback - but at this point, we should use a more generic approach
                container_name = "$(docker ps --format '{{.Names}}' | grep -E '(alunai|clarity|mcp)' | head -1 || echo 'current-container')"
                logger.info(f"ðŸ” DEBUG: Using shell detection fallback")
            
            # Create hook configuration that executes via Docker container
            hook_config = {
                "hooks": {
                    "UserPromptSubmit": [
                        {
                            "matcher": "*",
                            "hooks": [
                                {
                                    "type": "command",
                                    "command": f"docker exec {container_name} python /app/clarity/mcp/hook_analyzer.py --prompt-submit --prompt='{{prompt}}'",
                                    "timeout_ms": 2000,
                                    "continue_on_error": True,
                                    "modify_prompt": True
                                }
                            ]
                        }
                    ]
                },
                "metadata": {
                    "created_by": "mcp-alunai-clarity",
                    "version": "2.0.0",
                    "description": "MCP auto-capture hooks using Docker container execution",
                    "created_at": datetime.now().isoformat(),
                    "container_name": container_name
                }
            }
            
            # Add hooks configuration to existing config file (check both possible names and locations)
            # Users may have config files in different locations depending on their setup
            possible_config_paths = [
                "/app/.claude/alunai-clarity/config.json",
                "/app/.claude/alunai-clarity/default_config.json",
                "/app/data/config.json",
                "/app/data/default_config.json"
            ]
            
            config_path = None
            for path in possible_config_paths:
                if os.path.exists(path):
                    config_path = path
                    break
            
            if config_path and os.path.exists(config_path):
                # Read existing config
                with open(config_path, 'r') as f:
                    existing_config = json.load(f)
                
                # Add hooks section to existing config
                existing_config["claude_code_hooks"] = hook_config
                
                # Write back the merged config
                with open(config_path, 'w') as f:
                    json.dump(existing_config, f, indent=2)
                
                # Create hooks.json in the mounted data directory
                hooks_path = "/app/data/hooks.json"
                with open(hooks_path, 'w') as f:
                    json.dump(hook_config, f, indent=2)
                
                logger.info(f"âœ… Claude Code hooks added to config: {config_path}")
                logger.info(f"âœ… Claude Code hooks.json created: {hooks_path}")
                logger.info(f"âœ… Hooks will execute via Docker container: {container_name}")
            else:
                logger.warning(f"No config file found at any of these paths: {possible_config_paths}, cannot add hooks")
            
        except Exception as e:
            logger.warning(f"Failed to setup Claude Code hooks immediately: {e}")
            logger.info("Hooks will be configured during lazy initialization instead")
    
    async def _lazy_initialize_domains(self) -> None:
        """Initialize domains lazily when first memory operation is called."""
        logger.info(f"ðŸ” DEBUG: _lazy_initialize_domains called, _domains_initialized={self._domains_initialized}")
        
        # Return early if already initialized
        if self._domains_initialized:
            return
        
        # Initialize lock if needed
        if self._init_lock is None:
            import asyncio
            self._init_lock = asyncio.Lock()
        
        # Use lock to prevent concurrent initialization
        async with self._init_lock:
            # Double-check pattern: another call might have initialized while we waited
            if self._domains_initialized:
                logger.info("ðŸ” DEBUG: Domains already initialized by another call")
                return
            
            logger.info("ðŸ” DEBUG: Performing domain initialization under lock")
            logger.info(f"ðŸ” DEBUG: _quick_start_mode={self._quick_start_mode}")
            if self._quick_start_mode:
                logger.info("Performing quick-start initialization (essential services only)")
                await self._quick_start_initialize()
            else:
                logger.info("Performing full lazy domain initialization on first memory operation")
                logger.info(f"ðŸ” DEBUG: About to call domain_manager.initialize()")
                await self.domain_manager.initialize()
                logger.info(f"ðŸ” DEBUG: domain_manager.initialize() completed")
                
                # Initialize AutoCode hooks if enabled (done after domains are ready)
                autocode_enabled = self.config.get("autocode", {}).get("enabled", True)
                logger.info(f"ðŸ” DEBUG: AutoCode enabled={autocode_enabled}")
                if autocode_enabled:
                    logger.info(f"ðŸ” DEBUG: About to initialize AutoCode hooks")
                    try:
                        from clarity.autocode.hooks import AutoCodeHooks
                        from clarity.autocode.server import AutoCodeServerExtension
                        from clarity.autocode.hook_manager import HookManager, HookRegistry
                        
                        logger.info(f"ðŸ” DEBUG: Creating AutoCodeHooks instance")
                        self.autocode_hooks = AutoCodeHooks(self.domain_manager)
                        logger.info(f"ðŸ” DEBUG: Creating AutoCodeServerExtension instance")
                        self.autocode_server = AutoCodeServerExtension(self.domain_manager, self.autocode_hooks)
                        
                        # Initialize hook manager
                        logger.info(f"ðŸ” DEBUG: Creating HookManager instance")
                        self.hook_manager = HookManager(self.domain_manager, self.autocode_hooks)
                        logger.info(f"ðŸ” DEBUG: Registering HookManager")
                        HookRegistry.register_manager(self.hook_manager)
                        
                        # Initialize MCP hooks for Claude Code integration (includes auto-capture)
                        # Skip if already initialized early
                        if not hasattr(self, 'mcp_hooks') or self.mcp_hooks is None:
                            logger.info(f"ðŸ” DEBUG: About to initialize MCP hooks for Claude Code integration")
                            try:
                                from clarity.autocode.mcp_hooks import MCPAwarenessHooks
                                self.mcp_hooks = MCPAwarenessHooks(self.domain_manager)
                                await self.mcp_hooks.initialize()
                                logger.info("MCP hooks initialized - Claude Code integration enabled")
                            except Exception as e:
                                logger.warning(f"Failed to initialize MCP hooks: {e}")
                        else:
                            logger.info("ðŸ” DEBUG: MCP hooks already initialized early, skipping")
                        
                        logger.info("AutoCode hooks, server extensions, and hook manager initialized lazily")
                    except ImportError as e:
                        logger.warning(f"AutoCode components not available: {e}")
                else:
                    logger.info(f"ðŸ” DEBUG: AutoCode disabled, skipping hook initialization")
            
            self._domains_initialized = True
            logger.info("ðŸ” DEBUG: Domain initialization completed")
    
    async def _quick_start_initialize(self) -> None:
        """Initialize only essential domains for quick start mode."""
        logger.info("Initializing essential domains only (persistence and temporal)")
        
        # Initialize only essential domains for basic memory operations
        await self.domain_manager.persistence_domain.initialize()
        logger.info("Persistence domain initialized")
        
        await self.domain_manager.temporal_domain.initialize()
        logger.info("Temporal domain initialized")
        
        # Initialize episodic and semantic domains for basic content processing
        await self.domain_manager.episodic_domain.initialize()
        logger.info("Episodic domain initialized")
        
        await self.domain_manager.semantic_domain.initialize()
        logger.info("Semantic domain initialized")
        
        # Skip AutoCode domain, hooks, pattern detection, session analysis, etc.
        logger.info("Skipping AutoCode domain, hooks, and advanced analytics in quick-start mode")
        logger.info("Essential domains initialized - basic memory operations available")
